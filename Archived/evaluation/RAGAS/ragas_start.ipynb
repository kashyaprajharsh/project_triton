{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Topic Adherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Requires predefined set of topics. Provided in ``` reference_topics ```\n",
    " \n",
    "\n",
    "### **class MultiTurnSample**\n",
    "\n",
    " What is this class?\n",
    "\n",
    "    - A class designed to represent conversational data samples that involve multiple back-and-forth exchanges (turns) between users and AI. \n",
    "\n",
    " What is it doing?\n",
    "\n",
    "    - It organizes and validates conversational data between human and AI\n",
    "\n",
    "    - Provides a structured format for evaluation:\n",
    "\n",
    "            - evaluations: topic adherence, tool usage evaluation, answer accuracy.\n",
    "\n",
    "            - ```\n",
    "\n",
    "            class MultiTurnSample(BaseSample):\n",
    "                user_input: t.List[t.Union[HumanMessage, AIMessage, ToolMessage]]\n",
    "                # Each message type has specific attributes:\n",
    "                # HumanMessage: content\n",
    "                # AIMessage: content, tool_calls\n",
    "                # ToolMessage: content\n",
    "                # These fields provide \"ground truth\" for evaluation\n",
    "                reference: t.Optional[str] = None  # Expected answer\n",
    "                reference_tool_calls: t.Optional[t.List[ToolCall]] = None  # Expected tool usage\n",
    "                reference_topics: t.Optional[t.List[str]] = None  # Expected conversation topics\n",
    "                rubrics: t.Optional[t.Dict[str, str]] = None  # Evaluation criteria\n",
    "\n",
    "            ````\n",
    "\n",
    "### **class TopicAdherenceScore**\n",
    "\n",
    "   What is this class:\n",
    "\n",
    "      - It evaluates how well a conversational AI system stays on topic during a multi-turn conversation by measuring topic adherence\n",
    "\n",
    "   What is it doing?\n",
    "\n",
    "      The class measures topic adherence through three main steps:\n",
    "         \n",
    "         a) Topic Extraction:\n",
    "         \n",
    "         Takes a conversation (user inputs and AI responses)\n",
    "         \n",
    "         Uses TopicExtractionPrompt to identify all topics discussed in the conversation\n",
    "         \n",
    "         Example: From a conversation about physics, it might extract topics like \"Einstein's theory of relativity\"\n",
    "         \n",
    "         b) Topic Refusal Check:\n",
    "\n",
    "         For each extracted topic, checks if the AI actually answered questions about it or refused to answer\n",
    "         Uses TopicRefusedPrompt to determine if the AI properly addressed each topic\n",
    "         Creates a boolean array where True means the topic was addressed\n",
    "\n",
    "         c) Topic Classification:\n",
    "\n",
    "         Compares extracted topics against reference topics (expected topics for the conversation)\n",
    "\n",
    "         Uses TopicClassificationPrompt to determine if each extracted topic matches the reference topics\n",
    "\n",
    "         Creates another boolean array of classifications\n",
    "\n",
    "         Finally, it calculates a score using one of three modes:\n",
    "\n",
    "               precision: How many of the addressed topics were relevant\n",
    "               \n",
    "               recall: How many of the relevant topics were addressed\n",
    "               \n",
    "               f1: A balanced score between precision and recall (default)\n",
    "         \n",
    "         The scoring formula uses:\n",
    "         \n",
    "               True positives: Topics that were both relevant and addressed\n",
    "               \n",
    "               False positives: Topics that were addressed but not relevant\n",
    "               \n",
    "               False negatives: Topics that were relevant but not addressed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHEN IS THIS TOOL APPLICABLE?**\n",
    "When chatbot is only expected to response to queries related to predefined domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertagarcia/Desktop/learning/hackathon/new-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "# Lagchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "# RAGAS\n",
    "from ragas.dataset_schema import  SingleTurnSample, MultiTurnSample, EvaluationDataset\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import TopicAdherenceScore\n",
    "from ragas.llms import LangchainLLMWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_4 = [\n",
    "HumanMessage(content=\"Can you provide me with details about Einstein's theory of relativity?\"),\n",
    "AIMessage(content=\"Sure, let me retrieve the relevant information for you.\", tool_calls=[\n",
    "    ToolCall(name=\"document_search\", args={\"query\": \"Einstein's theory of relativity\"})\n",
    "]),\n",
    "ToolMessage(content=\"Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\"),\n",
    "AIMessage(content=\"I found some documents on Einstein's theory of relativity. Which one would you like to know more about: 'Relativity: The Special and the General Theory' or 'General Theory of Relativity by A. Einstein'?\"),\n",
    "HumanMessage(content=\"Tell me about the 'General Theory of Relativity'.\"),\n",
    "AIMessage(content=\"Got it! Let me fetch more details from 'General Theory of Relativity by A. Einstein'.\", tool_calls=[\n",
    "    ToolCall(name=\"document_retrieve\", args={\"document\": \"General Theory of Relativity by A. Einstein\"})\n",
    "]),\n",
    "ToolMessage(content=\"The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\"),\n",
    "AIMessage(content=\"The 'General Theory of Relativity' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\"),\n",
    "HumanMessage(content=\"No, that's perfect. By the way, do you know any good recipes for a chocolate cake?\"),\n",
    "AIMessage(content=\"Sure! Let me find a simple and delicious recipe for a chocolate cake.\", tool_calls=[\n",
    "    ToolCall(name=\"recipe_search\", args={\"query\": \"chocolate cake recipe\"})\n",
    "]),\n",
    "ToolMessage(content=\"Here’s a popular recipe for a chocolate cake: Ingredients include flour, sugar, cocoa powder, eggs, milk, and butter. Instructions: Mix dry ingredients, add wet ingredients, and bake at 350°F for 30-35 minutes.\"),\n",
    "AIMessage(content=\"I found a great recipe for chocolate cake! Would you like the full details, or is that summary enough?\"),\n",
    "\n",
    "HumanMessage(content = \"Who is the current president of the US?\"),\n",
    "AIMessage(content = \"Sorry, I can only respond answers related to science\"),\n",
    "# AIMessage(content = \" Let me look into this\", tool_calls = [\n",
    "# ToolCall(name = \"politics_research\", args = {\"query\" : \"Who is the current president of the US?\"})]),\n",
    "#ToolMessage(content = \"The Current President of the US is Donal Trump\"),\n",
    "#AIMessage(content = \" Donald Trump won the 2024 presidential elections, he will be in office until 2028\") \n",
    "# AIMessage(content = \" Sorry, I can only respond answers related to science\")\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(user_input=sample_input_4, reference_topics=[\"science\"])\n",
    "scorer = TopicAdherenceScore(mode=\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666444444"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "langchain_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) \n",
    "# Wrapp llm\n",
    "ragas_llm = LangchainLLMWrapper(langchain_llm)\n",
    "\n",
    "# Create the scorer with the wrapped LLM\n",
    "scorer = TopicAdherenceScore(mode=\"precision\")\n",
    "scorer.llm = ragas_llm\n",
    "\n",
    "# Run the scoring\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = TopicAdherenceScore(mode=\"recall\")\n",
    "scorer.llm = ragas_llm\n",
    "\n",
    "# Run the scoring\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tool Call Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- tool names and arguments are compared using exact string matching. You can also use any ragas metrics (values between 0 and 1) as distance measure to identify if a retrieved context is relevant or not. For example,\n",
    "\n",
    "\n",
    "\n",
    "### **Class ToolCallAccuracy**\n",
    "\n",
    "1. What is this class\n",
    "\n",
    "    Metric evaluator to measure how accuratelly an AI system uses tools in a conversation by comparing the actual tool calls made against the reference tool calls\n",
    "\n",
    "2. What is it doing?\n",
    "\n",
    "    The class evaluates tool usage accuracy in three main aspects:\n",
    "\n",
    "    a. Sequence alignenment \n",
    "\n",
    "        checks if tools are called in the correct order. Uses is_sequence_aligned()\n",
    "\n",
    "    b. Tool name matching\n",
    "\n",
    "        check if name of tool call matches that of the reference (using exact string matching) \n",
    "\n",
    "    c. Argument accuracy\n",
    "\n",
    "        Uses _get_arg_score() to compare predicted arguments against reference arguments\n",
    "\n",
    "        By default, uses exact string matching for argument comparison\n",
    "\n",
    "        How is it calculating this argument score?\n",
    "\n",
    "            Takes the average of matching arguments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ToolCallAccuracy\n",
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "\n",
    "sample = [\n",
    "    HumanMessage(content=\"What's the weather like in New York right now?\"),\n",
    "    AIMessage(content=\"The current temperature in New York is 75°F and it's partly cloudy.\", tool_calls=[\n",
    "        ToolCall(name=\"weather_check\", args={\"location\": \"New York\"})\n",
    "    ]),\n",
    "    HumanMessage(content=\"Can you translate that to Celsius?\"),\n",
    "    AIMessage(content=\"Let me convert that to Celsius for you.\", tool_calls=[\n",
    "        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n",
    "    ]),\n",
    "    ToolMessage(content=\"75°F is approximately 23.9°C.\"),\n",
    "    AIMessage(content=\"75°F is approximately 23.9°C.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(\n",
    "    user_input=sample,\n",
    "    reference_tool_calls=[\n",
    "        ToolCall(name=\"weather_check\", args={\"location\": \"New York\"}),\n",
    "        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IF THE TOOLS ARE NOT CALLS IN NOT IN THE ORDER SPECIFIED IN THE reference_tool_calls THE SCORE IS ZERO!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = ToolCallAccuracy()\n",
    "scorer.llm = ragas_llm\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics._string import NonLLMStringSimilarity\n",
    "# You can also use any ragas metrics (values between 0 and 1) as distance measure to identify if a retrieved context \n",
    "# is relevant or not. For example,\n",
    "metric = ToolCallAccuracy()\n",
    "metric.arg_comparison_metric = NonLLMStringSimilarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Agent Goal Accuracy**\n",
    "\n",
    "binary metric, LLM judge to decide if 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **AgentGoalAccuracyWithReference**\n",
    "\n",
    "1. What is this class?\n",
    "\n",
    "It is a metric evaluator that measures how well an AI agent achieves the user's intended goal by comparing the actual outcome against a reference (expected) outcome.\n",
    "\n",
    "2. What is it doing?\n",
    "\n",
    "It performs this evaluation by:\n",
    "\n",
    "    a. Workflow analysis:\n",
    "\n",
    "        Takes conversation (human, ai messages, and tool calls) and:\n",
    "\n",
    "       - Uses InferGoalOutcomePrompt to extract:\n",
    "\n",
    "            - user_goal: The original objective\n",
    "\n",
    "            - end_state: What was actually achieved\n",
    "    \n",
    "    b. Outcome Comparison:\n",
    "\n",
    "        - Uses CompareOutcomePrompt to compare:\n",
    "                \n",
    "                - desired_outcome: The reference (expected) outcome\n",
    "                \n",
    "                - arrived_outcome: The actual end state\n",
    "\n",
    "        \n",
    "        - Returns a binary score:\n",
    "\n",
    "                - 1.0: The outcomes match\n",
    "                \n",
    "                - 0.0: The outcomes differ\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference\n",
    "\n",
    "\n",
    "sample = MultiTurnSample(user_input=[\n",
    "    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n",
    "    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n",
    "    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n",
    "    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n",
    "    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n",
    "    AIMessage(content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "],\n",
    "    reference=\"Table booked at chinese restaurant at 8 pm\")\n",
    "\n",
    "scorer = AgentGoalAccuracyWithReference()\n",
    "scorer.llm = ragas_llm\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgentGoalAccuracyWithoutReference\n",
    "\n",
    "\n",
    "1. What is this class?\n",
    "\n",
    "    AgentGoalAccuracyWithoutReference is a variant of the goal accuracy evaluator that doesn't require a predefined reference outcome. Instead, it compares the achieved outcome against the inferred user goal from the conversation itself.\n",
    "\n",
    "2. What is it doing?\n",
    "\n",
    "    The key difference is that this class:\n",
    "        \n",
    "        Extracts both the user's goal and the end state from the conversation\n",
    "        \n",
    "        Uses these extracted elements for comparison, rather than relying on a reference\n",
    "        \n",
    "        Key differences in the implementation:\n",
    "\n",
    "            - Extracts both the user's goal and the end state from the conversation\n",
    "            \n",
    "            - Uses these extracted elements for comparison, rather than relying on a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import AgentGoalAccuracyWithoutReference\n",
    "\n",
    "\n",
    "sample = MultiTurnSample(user_input=[\n",
    "    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n",
    "    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n",
    "    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n",
    "    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n",
    "    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n",
    "    AIMessage(content=\"Your table at nearest best Chinese restaurant is booked for 8:00pm. Enjoy your meal!\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "])\n",
    "\n",
    "scorer = AgentGoalAccuracyWithoutReference()\n",
    "scorer.llm = ragas_llm\n",
    "await scorer.multi_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
